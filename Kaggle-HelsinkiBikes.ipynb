{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data\n",
    "\n",
    "Import the [Helisinki Bikes dataset from Kaggle](https://www.kaggle.com/datasets/geometrein/helsinki-city-bikes) and supporting package libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "data = pd.read_csv('KaggleHelsinkiBikesDatabase.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data\n",
    "\n",
    "- Analyze dateset for cleaning needs.\n",
    "- Drop any rows containing null values.\n",
    "- Update date types as needed.\n",
    "- Split date values into date components stored in separate columns in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12157458 entries, 0 to 12157457\n",
      "Data columns (total 14 columns):\n",
      " #   Column                  Dtype  \n",
      "---  ------                  -----  \n",
      " 0   departure               object \n",
      " 1   return                  object \n",
      " 2   departure_id            object \n",
      " 3   departure_name          object \n",
      " 4   return_id               object \n",
      " 5   return_name             object \n",
      " 6   distance (m)            float64\n",
      " 7   duration (sec.)         float64\n",
      " 8   avg_speed (km/h)        float64\n",
      " 9   departure_latitude      float64\n",
      " 10  departure_longitude     float64\n",
      " 11  return_latitude         float64\n",
      " 12  return_longitude        float64\n",
      " 13  Air temperature (degC)  float64\n",
      "dtypes: float64(8), object(6)\n",
      "memory usage: 1.3+ GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "departure                     0\n",
       "return                        0\n",
       "departure_id                  0\n",
       "departure_name                0\n",
       "return_id                     0\n",
       "return_name                   0\n",
       "distance (m)                  0\n",
       "duration (sec.)               0\n",
       "avg_speed (km/h)           3550\n",
       "departure_latitude            0\n",
       "departure_longitude           0\n",
       "return_latitude               1\n",
       "return_longitude              1\n",
       "Air temperature (degC)    15902\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze data for cleaning needs\n",
    "data.info()\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 12138008 entries, 0 to 12157457\n",
      "Data columns (total 14 columns):\n",
      " #   Column                  Dtype  \n",
      "---  ------                  -----  \n",
      " 0   departure               object \n",
      " 1   return                  object \n",
      " 2   departure_id            object \n",
      " 3   departure_name          object \n",
      " 4   return_id               object \n",
      " 5   return_name             object \n",
      " 6   distance (m)            float64\n",
      " 7   duration (sec.)         float64\n",
      " 8   avg_speed (km/h)        float64\n",
      " 9   departure_latitude      float64\n",
      " 10  departure_longitude     float64\n",
      " 11  return_latitude         float64\n",
      " 12  return_longitude        float64\n",
      " 13  Air temperature (degC)  float64\n",
      "dtypes: float64(8), object(6)\n",
      "memory usage: 1.4+ GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "departure                 0\n",
       "return                    0\n",
       "departure_id              0\n",
       "departure_name            0\n",
       "return_id                 0\n",
       "return_name               0\n",
       "distance (m)              0\n",
       "duration (sec.)           0\n",
       "avg_speed (km/h)          0\n",
       "departure_latitude        0\n",
       "departure_longitude       0\n",
       "return_latitude           0\n",
       "return_longitude          0\n",
       "Air temperature (degC)    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop any rows with null values\n",
    "data.dropna(subset=['avg_speed (km/h)', 'return_latitude', 'return_longitude', 'Air temperature (degC)'], inplace=True)\n",
    "data.info()\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "departure                 datetime64[ns]\n",
       "return                    datetime64[ns]\n",
       "departure_id                      object\n",
       "departure_name                    object\n",
       "return_id                         object\n",
       "return_name                       object\n",
       "distance (m)                     float64\n",
       "duration (sec.)                  float64\n",
       "avg_speed (km/h)                 float64\n",
       "departure_latitude               float64\n",
       "departure_longitude              float64\n",
       "return_latitude                  float64\n",
       "return_longitude                 float64\n",
       "Air temperature (degC)           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update data types where necessary\n",
    "data['departure'] = pd.to_datetime(data['departure'])\n",
    "data['return'] = pd.to_datetime(data['return'])\n",
    "data['departure_id'] = data['departure_id'].astype(str)\n",
    "data['return_id'] = data['return_id'].astype(str)\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split date values into date components and store in separate columns\n",
    "data['dept_day'] = data['departure'].dt.day\n",
    "data['dept_month'] = data['departure'].dt.month\n",
    "data['dept_year'] = data['departure'].dt.year\n",
    "data['dept_hour'] = data['departure'].dt.hour\n",
    "data['dept_minute'] = data['departure'].dt.minute\n",
    "data['dept_second'] = data['departure'].dt.second\n",
    "data['dept_microsecond'] = data['departure'].dt.microsecond\n",
    "data['dept_nanosecond'] = data['departure'].dt.nanosecond\n",
    "\n",
    "data['ret_day'] = data['return'].dt.day\n",
    "data['ret_month'] = data['return'].dt.month\n",
    "data['ret_year'] = data['return'].dt.year\n",
    "data['ret_hour'] = data['return'].dt.hour\n",
    "data['ret_minute'] = data['return'].dt.minute\n",
    "data['ret_second'] = data['return'].dt.second\n",
    "data['ret_microsecond'] = data['return'].dt.microsecond\n",
    "data['ret_nanosecond'] = data['return'].dt.nanosecond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Anomalies\n",
    "\n",
    "After manually analyzing the dataset, performing external research about the dataset, and brainstorming potential target audiences and problem statements / hypothesises, I came up with two anomaly use cases to focus on. The following code checks for the two anomaly use cases in the dataset:\n",
    "\n",
    "1. Bike ride distance not recorded:\n",
    "    - In these cases the bike was moved to a different station but the distance was not recorded. There might be a malfunction with recording the distance on the bike that the vender needs to look into.\n",
    "    - Set new column as true if (1) the distance = 0 meters and (2) the departure_id and return_id are not the same.\n",
    "2. Bike quickly returned to the same station:\n",
    "    - In these cases the bike rider may have changed their mind about renting the bike. If an event like this occurs often in the same station, there may be a possibility that there are defective bikes in that station and the vender should pay a visit to the station to check on the bikes.\n",
    "    - Set new column as true if (1) the duration is less than 60 seconds and (2) depature_id and return_id are the same.\n",
    "\n",
    "Lastly, I combined the anomaly label columns into a single, combined label column. If either anomaly is found, the label should be set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set new column as true if (1) the distance = 0 meters and (2) the departure_id and return_id are not the same\n",
    "data['bike_malfunction_recording_distance'] = np.where((data['distance (m)'] == 0) & (data['departure_id'] != data['return_id']), 1, 0)\n",
    "\n",
    "# Set new column as true if (1) the duration is less than 60 seconds and (2) depature_id and return_id are the same\n",
    "data['bike_malfunction_no_rides'] = np.where((data['duration (sec.)'] < 60) & (data['departure_id'] == data['return_id']), 1, 0)\n",
    "\n",
    "# Combine anomalies into a single column. If either anomaly is found, the label should be set to 1.\n",
    "data['combined_anomalies'] = np.where((data['bike_malfunction_recording_distance'] == 1) | (data['bike_malfunction_no_rides'] == 1), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total bike rides: 12138008\n",
      "Bike rides with no distance recorded: 211897; 0.017457312600222374 || where (1) the distance = 0 meters and (2) the departure_id and return_id are not the same\n",
      "Bikes quickly returned: 243110; 0.020028821862697735 || where (1) the duration is less than 60 seconds and (2) depature_id and return_id are the same\n",
      "Combined anoamlies found: 455007; 0.03748613446292011\n"
     ]
    }
   ],
   "source": [
    "# Calculate total rows for entire dataset and anomalies\n",
    "bike_rides_num_rows = len(data.index)\n",
    "bike_malfunction_recording_distance_num_rows = len(data[data['bike_malfunction_recording_distance'] == 1].index)\n",
    "bike_malfunction_no_rides_num_rows = len(data[data['bike_malfunction_no_rides'] == 1].index)\n",
    "combined_anomalies_num_rows = len(data[data['combined_anomalies'] == 1].index)\n",
    "\n",
    "print(f\"Total bike rides: {bike_rides_num_rows}\")\n",
    "print(f\"Bike rides with no distance recorded: {bike_malfunction_recording_distance_num_rows}; {np.divide(bike_malfunction_recording_distance_num_rows, bike_rides_num_rows)} || where (1) the distance = 0 meters and (2) the departure_id and return_id are not the same\")\n",
    "print(f\"Bikes quickly returned: {bike_malfunction_no_rides_num_rows}; {np.divide(bike_malfunction_no_rides_num_rows, bike_rides_num_rows)} || where (1) the duration is less than 60 seconds and (2) depature_id and return_id are the same\")\n",
    "print(f\"Combined anoamlies found: {combined_anomalies_num_rows}; {np.divide(combined_anomalies_num_rows, bike_rides_num_rows)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Data\n",
    "\n",
    "Prepare dataset for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding station names, station IDs, and departure/return dates & times\n",
    "encoder = LabelEncoder()\n",
    "data['departure_id'] = encoder.fit_transform(data['departure_id'])\n",
    "data['return_id'] = encoder.fit_transform(data['return_id'])\n",
    "data['departure_name'] = encoder.fit_transform(data['departure_name'])\n",
    "data['return_name'] = encoder.fit_transform(data['return_name'])\n",
    "data['departure'] = encoder.fit_transform(data['departure'])\n",
    "data['return'] = encoder.fit_transform(data['return'])\n",
    "data = pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation/testing datasets\n",
    "bike_rides_data_attributes = data.drop(['bike_malfunction_recording_distance', 'bike_malfunction_no_rides', 'combined_anomalies', 'departure', 'return'], axis = 1)\n",
    "bike_rides_anomaly_data_labels = data['combined_anomalies']\n",
    "bike_rides_anomaly_malfunction_recording_distance_data_labels = data['bike_malfunction_recording_distance']\n",
    "bike_rides_anomaly_malfunction_no_rides_data_labels = data['bike_malfunction_no_rides']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(bike_rides_data_attributes, bike_rides_anomaly_data_labels, test_size = 0.3, random_state = 42)\n",
    "\n",
    "X_train_dist, X_test_dist, y_train_dist, y_test_dist = train_test_split(bike_rides_data_attributes, bike_rides_anomaly_malfunction_recording_distance_data_labels, test_size = 0.3, random_state = 42)\n",
    "X_train_rides, X_test_rides, y_train_rides, y_test_rides = train_test_split(bike_rides_data_attributes, bike_rides_anomaly_malfunction_recording_distance_data_labels, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Gaussian Naive Bayes\n",
    "\n",
    "Train and test the Gaussian Naive Bayes model with training and testing datasets.\n",
    "Spliting the date values into their date components resulted in different accuracy scores.\n",
    "- Date values left as single value resulted in an accuracy score of 0.9624424981250359\n",
    "- Date values split into year, month, day, hour, minute, and second (as well any further) resulted in an accuracy score of 0.9624441458415891"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9624441458415891\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "pred_gnb = gnb.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, pred_gnb, normalize=True, sample_weight=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Decision Tree Classifier\n",
    "\n",
    "Train and test the Decision Tree Classifier model with training and testing datasets.\n",
    "Spliting the date values into their date components resulted in different accuracy scores.\n",
    "- Date values left as single value resulted in an accuracy score of 0.9994285169754625\n",
    "- Date values split into year, month, day, hour, minute, and second resulted in an accuracy score of 0.9993843032479514\n",
    "- Date values split into year, month, day, hour, minute, second, microsecond resulted in an accuracy score of 0.9993966611221005\n",
    "- Date values split into year, month, day, hour, minute, second, microsecond, and nanosecond resulted in an accuracy score of 0.9993972103609515"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9993972103609515\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "pred_clf = clf.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, pred_clf, normalize=True, sample_weight=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import tree\n",
    "\n",
    "# plt.figure().set_figwidth(7)\n",
    "# plt.figure().set_figheight(150)\n",
    "# tree.plot_tree(clf, rounded=True, fontsize=14)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
